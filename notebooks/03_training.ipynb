{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "421c5590",
   "metadata": {},
   "source": [
    "YOLO Model Training Pipeline\n",
    "\n",
    "This notebook executes the complete training workflow for YOLOv8n object detection model using Pascal VOC 2012 dataset. Trains on real-world dataset with configuration defined in notebook 02, tracks metrics with MLflow, and validates trained model.\n",
    "\n",
    "Training Workflow:\n",
    "1. Load model configuration from notebook 02\n",
    "2. Initialize YOLO model with pretrained COCO weights\n",
    "3. Execute training on Pascal VOC 2012 dataset (3000-5000 filtered images)\n",
    "4. Track training progress and metrics with MLflow\n",
    "5. Validate trained model on validation dataset\n",
    "6. Register best model to MLflow Model Registry\n",
    "7. Verify best.pt checkpoint ready for prediction phase\n",
    "\n",
    "Training Duration:\n",
    "- GPU: 30-60 minutes\n",
    "- CPU: 3-4 hours\n",
    "\n",
    "Dataset: Pascal VOC 2012 (~70% train, 15% val, 15% test split)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89c54676",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YOLO TRAINING PIPELINE\n",
      "============================================================\n",
      "Device: cpu\n",
      "Seed: 42\n",
      "Data: ..\\data\n",
      "Models: ..\\models\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "from pathlib import Path\n",
    "from ultralytics import YOLO\n",
    "import shutil\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "\n",
    "# MLflow setup\n",
    "mlflow.set_tracking_uri('file:///mlruns')\n",
    "mlflow.set_experiment('yolo_3class_detection')\n",
    "\n",
    "# Paths\n",
    "DATA_DIR = Path('../data')\n",
    "MODELS_DIR = Path('../models')\n",
    "MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"YOLO TRAINING PIPELINE\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Device: {torch.device('cuda' if torch.cuda.is_available() else 'cpu')}\")\n",
    "print(f\"Seed: {SEED}\")\n",
    "print(f\"Data: {DATA_DIR}\")\n",
    "print(f\"Models: {MODELS_DIR}\")\n",
    "print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8013f72",
   "metadata": {},
   "source": [
    "Stage 1: Environment Initialization and MLflow Setup\n",
    "\n",
    "This stage sets up the training environment with reproducibility settings and experiment tracking configuration.\n",
    "\n",
    "Components:\n",
    "- PyTorch and NumPy: Deep learning framework and numerical operations\n",
    "- MLflow: Experiment tracking and model versioning\n",
    "- YOLO: Ultralytics YOLOv8 object detection framework\n",
    "\n",
    "Reproducibility:\n",
    "- Fixed seeds (42) for NumPy, PyTorch CPU, and CUDA GPU\n",
    "- Ensures identical results across different runs and machines\n",
    "\n",
    "MLflow configuration:\n",
    "- Sets tracking URI to local mlruns directory\n",
    "- Creates experiment named \"yolo_3class_detection\"\n",
    "- All training metrics will be logged and retrievable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6125e9cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[1] Loading Configuration\n",
      "------------------------------------------------------------\n",
      "Model: yolov8n\n",
      "Classes: 3 (person, car, dog)\n",
      "\n",
      "Training config:\n",
      "  epochs: 50\n",
      "  batch_size: 16\n",
      "  imgsz: 416\n",
      "  patience: 10\n",
      "  device: cpu\n",
      "  seed: 42\n",
      "  lr0: 0.01\n",
      "  lrf: 0.01\n",
      "  momentum: 0.937\n",
      "  weight_decay: 0.0005\n",
      "  warmup_epochs: 3.0\n",
      "  warmup_momentum: 0.8\n"
     ]
    }
   ],
   "source": [
    "# Configuration (from notebook 02)\n",
    "MODEL_NAME = 'yolov8n'\n",
    "PRETRAINED_WEIGHTS = 'yolov8n.pt'\n",
    "NUM_CLASSES = 3\n",
    "CLASS_NAMES = ['person', 'car', 'dog']\n",
    "\n",
    "TRAINING_CONFIG = {\n",
    "    'epochs': 50,\n",
    "    'batch_size': 16,\n",
    "    'imgsz': 416,\n",
    "    'patience': 10,\n",
    "    'device': 0 if torch.cuda.is_available() else 'cpu',\n",
    "    'seed': SEED,\n",
    "    'lr0': 0.01,\n",
    "    'lrf': 0.01,\n",
    "    'momentum': 0.937,\n",
    "    'weight_decay': 0.0005,\n",
    "    'warmup_epochs': 3.0,\n",
    "    'warmup_momentum': 0.8,\n",
    "}\n",
    "\n",
    "print(\"\\n[1] Loading Configuration\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(f\"Classes: {NUM_CLASSES} ({', '.join(CLASS_NAMES)})\")\n",
    "print(f\"\\nTraining config:\")\n",
    "for key, value in TRAINING_CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c669642",
   "metadata": {},
   "source": [
    "Stage 2: Load Configuration from Notebook 02\n",
    "\n",
    "This stage replicates the model and training configuration defined in notebook 02.\n",
    "\n",
    "Configuration includes:\n",
    "- Model name and pretrained weights\n",
    "- Dataset specification (3 classes)\n",
    "- All training hyperparameters\n",
    "\n",
    "This duplication ensures that notebook 03 is self-contained and can be executed independently after notebook 02."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8a7265d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[2] Starting Training\n",
      "------------------------------------------------------------\n",
      "Ultralytics 8.4.9  Python-3.10.0 torch-2.10.0+cpu CPU (AMD Ryzen 9 5900XT 16-Core Processor)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0magnostic_nms=False, amp=True, angle=1.0, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=..\\data\\data.yaml, degrees=0.0, deterministic=True, device=cpu, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, end2end=None, epochs=50, erasing=0.4, exist_ok=True, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=416, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8n.pt, momentum=0.937, mosaic=1.0, multi_scale=0.0, name=yolo_run, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=10, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, rle=1.0, save=True, save_conf=False, save_crop=False, save_dir=C:\\Users\\mlata\\Documents\\iajordy2\\runs\\detect\\yolo_run, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=42, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
      "Overriding model.yaml nc=80 with nc=3\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    751897  ultralytics.nn.modules.head.Detect           [3, 16, None, [64, 128, 256]] \n",
      "Model summary: 130 layers, 3,011,433 parameters, 3,011,417 gradients, 8.2 GFLOPs\n",
      "\n",
      "Transferred 319/355 items from pretrained weights\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 968.8335.0 MB/s, size: 103.6 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning C:\\Users\\mlata\\Documents\\iajordy2\\data\\labels\\train.cache... 2026 images, 0 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 2026/2026  0.0s\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 871.2212.1 MB/s, size: 94.1 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\mlata\\Documents\\iajordy2\\data\\labels\\val.cache... 434 images, 0 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 434/434  0.0s\n",
      "Plotting labels to C:\\Users\\mlata\\Documents\\iajordy2\\runs\\detect\\yolo_run\\labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001429, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mlogging run_id(e6ae72c9dcf0477a9dc73ea9711c2b7a) to file:///mlruns\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mdisable with 'yolo settings mlflow=False'\n",
      "WARNING \u001b[34m\u001b[1mMLflow: \u001b[0mFailed to initialize: Changing param values is not allowed. Param with key='model' was already logged with value='yolov8n' for run ID='e6ae72c9dcf0477a9dc73ea9711c2b7a'. Attempted logging new value 'yolov8n.pt'.\n",
      "WARNING \u001b[34m\u001b[1mMLflow: \u001b[0mNot tracking this run\n",
      "Image sizes 416 train, 416 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mC:\\Users\\mlata\\Documents\\iajordy2\\runs\\detect\\yolo_run\u001b[0m\n",
      "Starting training for 50 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       1/50         0G      1.195      1.842      1.239         66        416: 100% ━━━━━━━━━━━━ 127/127 1.1it/s 1:590.8ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 14/14 1.7it/s 8.1s0.6s\n",
      "                   all        434       1040       0.64      0.513      0.587      0.368\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       2/50         0G      1.352      1.594      1.346         63        416: 100% ━━━━━━━━━━━━ 127/127 1.1it/s 1:560.7ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 14/14 1.7it/s 8.2s0.6s\n",
      "                   all        434       1040      0.546      0.496      0.448      0.241\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       3/50         0G      1.393      1.633      1.395         72        416: 100% ━━━━━━━━━━━━ 127/127 1.1it/s 1:540.8ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 14/14 1.7it/s 8.2s0.6s\n",
      "                   all        434       1040      0.455      0.301      0.316      0.162\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       4/50         0G      1.419      1.623      1.403         35        416: 100% ━━━━━━━━━━━━ 127/127 1.1it/s 1:540.8ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 14/14 1.8it/s 7.9s0.6s\n",
      "                   all        434       1040      0.618       0.51      0.499      0.255\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       5/50         0G      1.407      1.556      1.397         42        416: 100% ━━━━━━━━━━━━ 127/127 1.1it/s 1:540.7ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 14/14 1.8it/s 7.8s0.6s\n",
      "                   all        434       1040      0.556      0.424      0.472      0.269\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       6/50         0G      1.372      1.546      1.388         48        416: 100% ━━━━━━━━━━━━ 127/127 1.1it/s 1:560.7ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 14/14 1.8it/s 7.9s0.6s\n",
      "                   all        434       1040      0.603      0.517      0.541      0.314\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       7/50         0G      1.343      1.468       1.37         66        416: 100% ━━━━━━━━━━━━ 127/127 1.1it/s 1:550.8ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 14/14 1.8it/s 8.0s0.6s\n",
      "                   all        434       1040      0.663      0.536      0.597      0.348\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       8/50         0G      1.332      1.429      1.363         39        416: 100% ━━━━━━━━━━━━ 127/127 1.1it/s 1:550.8ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 14/14 1.8it/s 7.9s0.6s\n",
      "                   all        434       1040      0.626      0.537      0.577      0.339\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       9/50         0G      1.298      1.403      1.344         24        416: 100% ━━━━━━━━━━━━ 127/127 1.1it/s 1:550.8ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 14/14 1.8it/s 8.0s0.6s\n",
      "                   all        434       1040      0.661      0.504      0.558      0.324\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      10/50         0G      1.286      1.344      1.327         45        416: 100% ━━━━━━━━━━━━ 127/127 1.1it/s 1:570.9ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 14/14 1.4it/s 10.1s.8s\n",
      "                   all        434       1040      0.647      0.491      0.556      0.324\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      11/50         0G      1.272      1.354      1.328         75        416: 100% ━━━━━━━━━━━━ 127/127 1.2s/it 2:291.0ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 14/14 1.5it/s 9.2s0.7s\n",
      "                   all        434       1040      0.678      0.529        0.6      0.363\n",
      "\u001b[34m\u001b[1mEarlyStopping: \u001b[0mTraining stopped early as no improvement observed in last 10 epochs. Best results observed at epoch 1, best model saved as best.pt.\n",
      "To update EarlyStopping(patience=10) pass a new patience value, i.e. `patience=300` or use `patience=0` to disable EarlyStopping.\n",
      "\n",
      "11 epochs completed in 0.388 hours.\n",
      "Optimizer stripped from C:\\Users\\mlata\\Documents\\iajordy2\\runs\\detect\\yolo_run\\weights\\last.pt, 6.2MB\n",
      "Optimizer stripped from C:\\Users\\mlata\\Documents\\iajordy2\\runs\\detect\\yolo_run\\weights\\best.pt, 6.2MB\n",
      "\n",
      "Validating C:\\Users\\mlata\\Documents\\iajordy2\\runs\\detect\\yolo_run\\weights\\best.pt...\n",
      "Ultralytics 8.4.9  Python-3.10.0 torch-2.10.0+cpu CPU (AMD Ryzen 9 5900XT 16-Core Processor)\n",
      "Model summary (fused): 73 layers, 3,006,233 parameters, 0 gradients, 8.1 GFLOPs\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 14/14 2.0it/s 7.1s0.5s\n",
      "                   all        434       1040       0.64      0.513      0.588      0.368\n",
      "                person        317        712      0.588      0.632      0.638       0.37\n",
      "                   car        113        243       0.72      0.593      0.703      0.479\n",
      "                   dog         64         85      0.611      0.314      0.422      0.255\n",
      "Speed: 0.4ms preprocess, 12.4ms inference, 0.0ms loss, 0.8ms postprocess per image\n",
      "Results saved to \u001b[1mC:\\Users\\mlata\\Documents\\iajordy2\\runs\\detect\\yolo_run\u001b[0m\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mresults logged to file:///mlruns\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mdisable with 'yolo settings mlflow=False'\n",
      "\n",
      "✓ Best model found at: ..\\runs\\detect\\yolo_run\\weights\\best.pt\n",
      "✓ Best model copied to: ..\\models\\best.pt\n",
      "Training run completed\n"
     ]
    }
   ],
   "source": [
    "# End any existing MLflow run\n",
    "if mlflow.active_run():\n",
    "    mlflow.end_run()\n",
    "\n",
    "# Initialize model\n",
    "model = YOLO(PRETRAINED_WEIGHTS)\n",
    "\n",
    "# Start MLflow run\n",
    "mlflow.start_run(run_name='yolo_training_run')\n",
    "\n",
    "# Log parameters\n",
    "mlflow.log_params({\n",
    "    'model': MODEL_NAME,\n",
    "    'num_classes': NUM_CLASSES,\n",
    "    'classes': ', '.join(CLASS_NAMES),\n",
    "    **TRAINING_CONFIG\n",
    "})\n",
    "\n",
    "print(\"\\n[2] Starting Training\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Train - YOLO saves to runs/ by default\n",
    "results = model.train(\n",
    "    data=str(DATA_DIR / 'data.yaml'),\n",
    "    epochs=TRAINING_CONFIG['epochs'],\n",
    "    imgsz=TRAINING_CONFIG['imgsz'],\n",
    "    batch=TRAINING_CONFIG['batch_size'],\n",
    "    device=TRAINING_CONFIG['device'],\n",
    "    patience=TRAINING_CONFIG['patience'],\n",
    "    seed=TRAINING_CONFIG['seed'],\n",
    "    save=True,\n",
    "    exist_ok=True,\n",
    "    name='yolo_run',\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# YOLO saves to runs/detect/yolo_run by default\n",
    "best_model_path = Path('../runs/detect/yolo_run/weights/best.pt')\n",
    "\n",
    "if best_model_path.exists():\n",
    "    # Copy best model to models directory for easy access\n",
    "    MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    dst_best_model = MODELS_DIR / 'best.pt'\n",
    "    shutil.copy2(str(best_model_path), str(dst_best_model))\n",
    "    \n",
    "    mlflow.log_artifact(str(best_model_path), artifact_path='models')\n",
    "    mlflow.log_metric('training_completed', 1)\n",
    "    print(f\"\\n✓ Best model found at: {best_model_path}\")\n",
    "    print(f\"✓ Best model copied to: {dst_best_model}\")\n",
    "else:\n",
    "    mlflow.log_metric('training_completed', 0)\n",
    "    print(f\"\\n✗ Best model NOT found at {best_model_path}\")\n",
    "    print(f\"Expected path: {best_model_path}\")\n",
    "    # Try to find where YOLO actually saved it\n",
    "    import glob\n",
    "    search_paths = list(Path('../runs').glob('**/best.pt'))\n",
    "    if search_paths:\n",
    "        print(f\"Found best.pt at: {search_paths[0]}\")\n",
    "\n",
    "mlflow.end_run()\n",
    "print(\"Training run completed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ebc8d74",
   "metadata": {},
   "source": [
    "Stage 3: Execute Training with MLflow Logging\n",
    "\n",
    "This is the core training stage. The YOLO model is initialized with pretrained COCO weights and fine-tuned on the custom 3-class dataset.\n",
    "\n",
    "Training flow:\n",
    "1. Load pretrained YOLOv8n weights (trained on COCO 80-class dataset)\n",
    "2. Start MLflow run for experiment tracking\n",
    "3. Execute model.train() with specified hyperparameters\n",
    "4. Save best model checkpoint to models/yolo_run/weights/best.pt\n",
    "5. Log training artifacts to MLflow\n",
    "6. End MLflow run\n",
    "\n",
    "Key points:\n",
    "- Transfer learning: Pretrained weights provide feature extraction knowledge\n",
    "- Early stopping: Stops if validation metric doesn't improve for 10 epochs\n",
    "- Best model: Saved checkpoint with best validation performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6de2d767",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found best model at: ..\\runs\\detect\\yolo_run\\weights\\best.pt\n",
      "Ultralytics 8.4.9  Python-3.10.0 torch-2.10.0+cpu CPU (AMD Ryzen 9 5900XT 16-Core Processor)\n",
      "Model summary (fused): 73 layers, 3,006,233 parameters, 0 gradients, 8.1 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.10.0 ms, read: 744.7280.7 MB/s, size: 87.4 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\mlata\\Documents\\iajordy2\\data\\labels\\val.cache... 434 images, 0 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 434/434  0.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 28/28 4.4it/s 6.4s0.2s\n",
      "                   all        434       1040      0.638      0.515      0.588      0.367\n",
      "Speed: 0.3ms preprocess, 11.1ms inference, 0.0ms loss, 0.9ms postprocess per image\n",
      "Results saved to \u001b[1mC:\\Users\\mlata\\Documents\\iajordy2\\runs\\detect\\val2\u001b[0m\n",
      "\n",
      "[3] Validation Metrics\n",
      "------------------------------------------------------------\n",
      "  mAP50: 0.5880\n",
      "  mAP50_95: 0.3675\n",
      "  precision: 0.6381\n",
      "  recall: 0.5148\n",
      "\n",
      "Validation completed\n"
     ]
    }
   ],
   "source": [
    "# Validation\n",
    "# Use best model from runs directory or models directory\n",
    "best_model_candidates = [\n",
    "    Path('../runs/detect/yolo_run/weights/best.pt'),  # Default YOLO location\n",
    "    Path('../models/best.pt'),  # Copied location\n",
    "]\n",
    "\n",
    "best_model_path = None\n",
    "for candidate in best_model_candidates:\n",
    "    if candidate.exists():\n",
    "        best_model_path = candidate\n",
    "        print(f\"Found best model at: {best_model_path}\")\n",
    "        break\n",
    "\n",
    "if best_model_path:\n",
    "    mlflow.start_run(run_name='yolo_validation_run')\n",
    "    \n",
    "    best_model = YOLO(str(best_model_path))\n",
    "    val_results = best_model.val(\n",
    "        data=str(DATA_DIR / 'data.yaml'),\n",
    "        imgsz=TRAINING_CONFIG['imgsz'],\n",
    "        batch=TRAINING_CONFIG['batch_size'],\n",
    "        device=TRAINING_CONFIG['device'],\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    mlflow.log_params({\n",
    "        'validation_dataset': 'val',\n",
    "        'model_checkpoint': 'best.pt'\n",
    "    })\n",
    "    \n",
    "    if hasattr(val_results, 'box') and val_results.box:\n",
    "        metrics = {\n",
    "            'mAP50': float(val_results.box.map50),\n",
    "            'mAP50_95': float(val_results.box.map),\n",
    "            'precision': float(val_results.box.p.mean()),\n",
    "            'recall': float(val_results.box.r.mean()),\n",
    "        }\n",
    "        for metric_name, metric_value in metrics.items():\n",
    "            mlflow.log_metric(metric_name, metric_value)\n",
    "        \n",
    "        print(\"\\n[3] Validation Metrics\")\n",
    "        print(\"-\" * 60)\n",
    "        for metric_name, metric_value in metrics.items():\n",
    "            print(f\"  {metric_name}: {metric_value:.4f}\")\n",
    "    \n",
    "    mlflow.end_run()\n",
    "    print(\"\\nValidation completed\")\n",
    "else:\n",
    "    print(\"\\n✗ Cannot validate: best.pt not found in any expected location\")\n",
    "    print(\"Expected locations:\")\n",
    "    for candidate in best_model_candidates:\n",
    "        print(f\"  - {candidate}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ee5408",
   "metadata": {},
   "source": [
    "Stage 4: Model Validation and Metrics Logging\n",
    "\n",
    "This stage validates the trained model on the validation dataset and logs performance metrics to MLflow.\n",
    "\n",
    "Validation process:\n",
    "1. Load best.pt model checkpoint\n",
    "2. Execute model.val() on validation dataset with same hyperparameters used in training\n",
    "3. Extract performance metrics from validation results\n",
    "\n",
    "Metrics computed:\n",
    "- mAP50: Mean Average Precision at IoU threshold 0.50\n",
    "- mAP50_95: Mean Average Precision averaged over IoU thresholds 0.50-0.95\n",
    "- precision: Proportion of detections that are correct\n",
    "- recall: Proportion of ground truth objects that are detected\n",
    "\n",
    "MLflow logs:\n",
    "- All metrics for experiment tracking and comparison\n",
    "- Model checkpoint path reference\n",
    "- Validation dataset specification"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
